---
title: "lab report knapsack"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lab report knapsack}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(Lab6)
library(profvis)
library(knitr)
library(parallel)
```

# Introduction

This vignette serves as the **lab report** for **Lab 6** in *Advanced R Programming*.

The purpose of this lab is to explore **algorithmic complexity** and **computational performance** using the **Knapsack problem**.


# The Knapsack Problem

The **Knapsack problem** is a classic *discrete optimization problem*.  
We have a knapsack that can carry a limited weight `W`, and we want to fill it with `n` items, each item `i` having a weight `wᵢ` and a value `vᵢ`.  
The goal is to **maximize the total value** of the selected items without exceeding the weight limit.

Formally:

$$
\text{maximize} \quad \sum_{i=1}^n v_i x_i \\
\text{subject to} \quad \sum_{i=1}^n w_i x_i \le W, \quad x_i \in \{0,1\}
$$

This problem is **NP-hard**, meaning that it’s at least as hard as the hardest problems in NP.  
No polynomial-time algorithm is known for solving it optimally (this is an open Millennium Prize problem).  
For background, see the [Wikipedia page on the Knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem).

---

# Data Generation

The data for this lab is generated as follows.  
To create larger datasets, you can increase the value of `n`.  
Be careful with the random number generator version to ensure reproducibility.

```{r}
# Ensure consistent random number generation
RNGversion(min(as.character(getRversion()), "3.5.3"))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")

# Number of items
n <- 2000

# Generate weights and values
knapsack_objects <- data.frame(
  w = sample(1:4000, size = n, replace = TRUE),
  v = runif(n = n, 0, 10000)
)

# Display first few rows
head(knapsack_objects)
```



# 1.1.2 Brute force search

*Question: How much time does it takes to run the algorithm for n = 16 objects?*


## How much time does it take for n = 16 objects?

To evaluate the performance of the brute-force algorithm, we can measure the execution time when solving a knapsack problem with **16 items**.  
Since the algorithm checks all \( 2^{16} = 65,536 \) possible combinations, it should still be feasible to compute — but noticeably slower than smaller cases.

```{r brute-force-timing}
# Generate data for 16 objects
RNGversion(min(as.character(getRversion()), "3.5.3"))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")

n <- 16
knapsack_objects <- data.frame(
  w = sample(1:4000, size = n, replace = TRUE),
  v = runif(n = n, 0, 10000)
)

# Measure computation time
system.time({
  result <- brute_force_knapsack(knapsack_objects, W = 3500)
})

result
```



# 1.1.3 Dynamic programming

*Question: How much time does it takes to run the algorithm for n = 500 objects?*

```{r dynamic-programming}
# Generate data for 500 objects
RNGversion(min(as.character(getRversion()), "3.5.3"))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")

n <- 500
knapsack_objects <- data.frame(
  w = sample(1:4000, size = n, replace = TRUE),
  v = runif(n = n, 0, 10000)
)

# Measure computation time
system.time({
  result <- knapsack_dynamic(knapsack_objects, W = 3500)
})

result
```

# 1.1.4 Greedy heuristic

*Question: How much time does it takes to run the algorithm for n = 1000000 objects?*

```{r greedy-knapsack}
# Generate data for 500 objects
RNGversion(min(as.character(getRversion()), "3.5.3"))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")

n <- 1000000
knapsack_objects <- data.frame(
  w = sample(1:4000, size = n, replace = TRUE),
  v = runif(n = n, 0, 10000)
)

# Measure computation time
system.time({
  result <- greedy_knapsack(knapsack_objects, W = 3500)
})

result
```

# 1.1.5 Test suits

*Justification that our algorithm is Greedy*

Our greedy_knapsack function implements the greedy approximation algorithm by first calculating the value-to-weight ratio for every item. It then sorts the items in descending order based on this ratio, ensuring the most 'efficient' items are considered first. Finally, it iterates through the sorted list, adding each item to the knapsack if it fits within the remaining weight capacity. This approach follows the definition of the greedy algorithm for the knapsack problem and has a computational complexity of O(n log n), dominated by the sorting step.

# 1.1.6 Profile your code and optimize your code

To identify performance bottlenecks, We used the `profvis` package as recommended in the lab instructions.

### Profiling Analysis

* **`brute_force_knapsack()`**: The profiler confirmed that the `for` loop, which iterates through all \( 2^n \) combinations, is the bottleneck. The operations inside are fast, but their sheer number makes the function slow.
* **`knapsack_dynamic()`**: The nested `for` loops used to fill the dynamic programming matrix accounted for must of the execution time. This is the computational core of the algorithm.
* **`greedy_knapsack()`**: The profiler highlighted two main time consumers: the `order()` call for sorting items by their value/weight ratio (the expected O(n log n) step) and the subsequent `for` loop used to fill the knapsack.

### Optimization of `greedy_knapsack`

The `for` loop in the `greedy_knapsack` function was a good candidate for optimization. We replaced the iterative approach with a more efficient, **vectorized** solution using `cumsum()`. This function calculates the cumulative weight of the sorted items in a single, fast operation, allowing us to select all valid items without an R-level loop.

*Question: What performance gain could you get by trying to improve your code?*

To quantify the performance gain, I benchmarked the original `for` loop implementation against the new, optimized `cumsum()` version using the `microbenchmark` package.

```{r optimization-benchmark}

greedy_knapsack_original <- function(x, W) {
  x$ratio <- x$v / x$w
  x$original_index <- seq_len(nrow(x))
  sorted_x <- x[order(x$ratio, decreasing = TRUE), ]
  total_weight <- 0
  total_value <- 0
  elements <- c()
  for (i in 1:nrow(sorted_x)) {
    item <- sorted_x[i, ]
    if (total_weight + item$w <= W) {
      total_weight <- total_weight + item$w
      total_value <- total_value + item$v
      elements <- c(elements, item$original_index)
    }
  }
  return(list(value = round(total_value), elements = sort(elements)))
}

n <- 1000
knapsack_objects_large <- data.frame(
  w = sample(1:4000, size = n, replace = TRUE),
  v = runif(n = n, 0, 10000)
)

microbenchmark::microbenchmark(
  original_loop = greedy_knapsack_original(knapsack_objects_large, W = 3500),
  optimized_cumsum = greedy_knapsack(knapsack_objects_large, W = 3500),
  times = 3
)
```


# 1.1.8 (*) Parallelize brute force search

*Question: What performance gain could you get by parallelizing brute force search?*

To answer the question we test some values for n and se what the result is:

```{r parallel-brute-force, message=FALSE, warning=FALSE}


suppressWarnings(RNGversion(min(as.character(getRversion()), "3.5.3")))
set.seed(42, kind = "Mersenne-Twister", normal.kind = "Inversion")

# measure runtime for given n
compare_runtime <- function(n, W = 3500) {
  knapsack_objects <- data.frame(
    w = sample(1:4000, size = n, replace = TRUE),
    v = runif(n = n, 0, 10000)
  )
  
  #  parallel set to FALSE
  seq_time <- system.time(
    seq_res <- brute_force_knapsack_parallel(knapsack_objects, W, parallel = FALSE)
  )[["elapsed"]]
  
  # parallel brute force
  par_time <- system.time(
    par_res <- brute_force_knapsack_parallel(knapsack_objects, W, parallel = TRUE)
  )[["elapsed"]]
  
  data.frame(
    n = n,
    Sequential = round(seq_time, 3),
    Parallel = round(par_time, 3)
  )
}

# compare for increasing n 
runtime_results <- do.call(rbind, lapply(c(12, 16, 20, 24 ), compare_runtime))
colnames(runtime_results) <- c("n", "Sequential (s)", "Parallel (s)")

knitr::kable(
  runtime_results,
  caption = "Comparison of sequential and parallel brute-force knapsack runtimes (in seconds).",
  align = "c",
  digits = 3
)

```


In the table we can se that for small problem sizes the sequential implementation outperforms the parallel version due to the time it takes of creating worker processes and data transfer between them. However, as n increases, the number of possible combinations grows exponentially ($2^n$), making the workload large enough to benefit from parallel processing.








